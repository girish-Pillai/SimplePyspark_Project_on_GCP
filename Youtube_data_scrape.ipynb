{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e9f837-c3f5-4518-ae65-96d70e59d206",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['SPARK_HOME'] = '/work/rc/g.pillai/spark-3.3.1-bin-hadoop3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40054db7-42ed-457a-97d6-d17fb7e70953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d624f306-7035-4734-824c-2ee22b200b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/12/18 22:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"YoutubeData\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "58e6b914-6498-401f-96c0-f1ab1c378496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Sample Python code for youtube.search.list\n",
    "# See instructions for running these code samples locally:\n",
    "# https://developers.google.com/explorer-help/code-samples#python\n",
    "\n",
    "import os\n",
    "\n",
    "import google_auth_oauthlib.flow\n",
    "import googleapiclient.discovery\n",
    "import googleapiclient.errors\n",
    "\n",
    "scopes = [\"https://www.googleapis.com/auth/youtube.force-ssl\"]\n",
    "\n",
    "def search_data():\n",
    "    # Disable OAuthlib's HTTPS verification when running locally.\n",
    "    # *DO NOT* leave this option enabled in production.\n",
    "    # os.environ[\"OAUTHLIB_INSECURE_TRANSPORT\"] = \"1\"\n",
    "\n",
    "    api_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'  #Use your API key here\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "\n",
    "    # Get credentials and create an API client\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "\n",
    "    request = youtube.search().list(\n",
    "        part=\"snippet\",\n",
    "        channelType=\"any\",\n",
    "        maxResults=50,\n",
    "        q=\"Pyspark using Airflow in GCP\",\n",
    "        order=\"relevance\",\n",
    "        videoDefinition=\"any\"\n",
    "    )\n",
    "    response = request.execute()\n",
    "\n",
    "    return response\n",
    "\n",
    "search_data = search_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0e064565-3336-4f9f-b158-241e9e6f5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_data(video_id):\n",
    "    api_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'  #Use your API key here\n",
    "    api_service_name = \"youtube\"\n",
    "    api_version = \"v3\"\n",
    "\n",
    "    youtube = googleapiclient.discovery.build(\n",
    "        api_service_name, api_version, developerKey=api_key)\n",
    "    \n",
    "\n",
    "    request = youtube.videos().list(\n",
    "        part = \"snippet\",\n",
    "        id = video_id\n",
    "    )\n",
    "    \n",
    "    response = request.execute()\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "23faba3e-8cf6-4bc2-92cd-afcb18a638c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_description(search_data):\n",
    "    desc_data = []\n",
    "    for item in search_data['items']:\n",
    "        try:\n",
    "            video_id = item['id']['videoId']\n",
    "            json_data = video_data(str(video_id))\n",
    "            description = json_data['items'][0]['snippet']['description']\n",
    "            tags = json_data['items'][0]['snippet']['tags']\n",
    "            desc_data.append({'videoId': video_id,\n",
    "                              'description': description,\n",
    "                              'tags' : tags\n",
    "                            })\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return desc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a919847c-8356-4dfe-9d3a-b2496a67d1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data_from_search = []\n",
    "for item in search_data['items']:\n",
    "    try:\n",
    "        video_id = item['id']['videoId']\n",
    "    except KeyError:\n",
    "        video_id = None\n",
    "\n",
    "    try:\n",
    "        title = item['snippet']['title']\n",
    "    except KeyError:\n",
    "        title = None\n",
    "\n",
    "    try:\n",
    "        channel_title = item['snippet']['channelTitle']\n",
    "    except KeyError:\n",
    "        channel_title = None\n",
    "\n",
    "    try:\n",
    "        published_at = item['snippet']['publishedAt']\n",
    "    except KeyError:\n",
    "        published_at = None\n",
    "\n",
    "    video_data_from_search.append({\n",
    "        'videoId': video_id,\n",
    "        'title': title,\n",
    "        'channelTitle': channel_title,\n",
    "        'publishedAt': published_at\n",
    "        # Add more fields as needed\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31a6aee7-133c-4d2d-ba90-e36818a12f03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_descriptions = get_description(search_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d3a169f4-6a0a-49d1-a6b7-7a9dc9cfa425",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|    videoId|               title|        channelTitle|        publishedAt|         description|                tags|date_created|\n",
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|2v9AKewyUEo|Learning Apache A...|         Soumil Shah|2021-01-24 15:05:38|Code :https://git...|[airflow, apche, ...|  2023-12-18|\n",
      "|5peQThvQmQk|Learn Apache Airf...|      Darshil Parmar|2023-10-07 13:00:04|Join My Data Engi...|[darshil parmar, ...|  2023-12-18|\n",
      "|7_HfVnjZ924|Installing Airflo...|       futureXskills|2023-01-14 19:47:29|Airflow Playlist ...|[airflow, apache ...|  2023-12-18|\n",
      "|DzxtCxi4YaA|Data Engineer Pro...|      Data with Marc|2023-08-08 14:11:37|Data Engineer Pro...|[data engineer, d...|  2023-12-18|\n",
      "|GqAcTrqKcrY|Realtime Data Str...|          CodeWithYu|2023-09-06 09:28:13|In this video, yo...|[Data Engineering...|  2023-12-18|\n",
      "|InDXPeg6r5U|How to Read and W...|Cloud & AI Analytics|2022-03-05 15:29:21|github url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|Jj6mp7Sam10|Dataproc in a minute|   Google Cloud Tech|2021-02-08 05:00:15|Learn more about ...|[GDS: Yes, what i...|  2023-12-18|\n",
      "|K9AnJ9_ZAXE|Airflow Tutorial ...|             coder2j|2022-06-05 22:00:07|Airflow Tutorial ...|[python, etl, dat...|  2023-12-18|\n",
      "|LkGFyi8S4Ys|Create a Dataproc...|Learn Cloud Compu...|2023-03-03 21:03:46|Please download t...|[Dataproc, Airflo...|  2023-12-18|\n",
      "|MhCuxTDlVkE|Airflow with DBT ...|      Data with Marc|2023-02-14 14:00:11|Airflow with DBT ...|[apache airflow, ...|  2023-12-18|\n",
      "|OBRv3t697sQ|GCP Composer | Ai...|Anjan GCP Data En...|2022-12-23 05:00:08|Code for educatio...|[gap, composer, a...|  2023-12-18|\n",
      "|PCg9AQNuK3E|Cloud Composer - ...|Cloud & AI Analytics|2023-03-05 19:54:42|GitHub url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|Q9eVezRq5pM|How to Read and W...|Cloud & AI Analytics|2022-12-23 10:08:32|GitHub url - http...|[Data Science, Go...|  2023-12-18|\n",
      "|TbdRQ-TmtgI|Apache Spark in 6...|      IBM Technology|2022-11-07 16:00:30|Learn more about ...|    [IBM, IBM Cloud]|  2023-12-18|\n",
      "|YgodScEIbOc|Create first Dag ...|        TechTrapture|2023-01-05 17:10:09|#gcp #googlecloud...|[composer, airflo...|  2023-12-18|\n",
      "|ZWgWiUH40qg|Apache Airflow Se...|              dataNX|2020-06-30 14:30:11|Hello,\\n\\nIn this...|[run airflow dag,...|  2023-12-18|\n",
      "|_lwrfxE2RtE|Google DataProc P...|           SkillCurb|2022-08-18 02:45:58|In this lab we wi...|[what is Dataproc...|  2023-12-18|\n",
      "|bHudgNDyltI|How to Read Diffe...|Cloud & AI Analytics|2022-05-16 23:33:35|github url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|d4cu_rzv4A8|GCP Cloud Compose...|Anjan GCP Data En...|2022-11-08 16:21:05|GCP Cloud Compose...|[GCP, composer, a...|  2023-12-18|\n",
      "|dLcDBs9ilf0|How to Run Data S...|   Google Cloud Tech|2023-04-07 16:26:49|Are you trying to...|[dataproc, compos...|  2023-12-18|\n",
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "search_schema = StructType([\n",
    "    StructField('videoId', StringType(), True),\n",
    "    StructField('title', StringType(), True),\n",
    "    StructField('channelTitle', StringType(), True),\n",
    "    StructField('publishedAt', StringType(), True)\n",
    "])\n",
    "\n",
    "desc_schema = StructType([\n",
    "    StructField('videoId', StringType(), True),\n",
    "    StructField('description', StringType(), True),\n",
    "    StructField('tags', ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "video_df = spark.createDataFrame(video_data_from_search, schema=search_schema)\n",
    "desc_df = spark.createDataFrame(video_descriptions, schema=desc_schema)\n",
    "\n",
    "spark_df = video_df.join(desc_df, 'videoId', 'inner')\n",
    "\n",
    "# Convert 'publishedAt' column to timestamp\n",
    "spark_df = spark_df.withColumn('publishedAt', to_timestamp('publishedAt', 'yyyy-MM-dd\\'T\\'HH:mm:ss\\'Z\\''))\n",
    "\n",
    "# Adding 'date_created' column with today's date\n",
    "spark_df = spark_df.withColumn('date_created', current_date())\n",
    "\n",
    "# Show the DataFrame\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "25e0dc77-ce44-45a6-b0ab-11940618e34a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8254b6b8-08f5-4521-8ff7-1ffed386369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the joined DataFrame as a CSV file\n",
    "output_path = \"/work/rc/g.pillai/learnSpark/twitter-airflow/youtube_sample_data\"  # Replace with your desired output path\n",
    "\n",
    "# Write the DataFrame to parquet table\n",
    "spark_df.write \\\n",
    "        .partitionBy(\"date_created\") \\\n",
    "        .parquet(output_path, mode=\"append\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e4133cb-e88b-41de-8b45-f02470385199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|    videoId|               title|        channelTitle|        publishedAt|         description|                tags|date_created|\n",
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "|2v9AKewyUEo|Learning Apache A...|         Soumil Shah|2021-01-24 15:05:38|Code :https://git...|[airflow, apche, ...|  2023-12-18|\n",
      "|5peQThvQmQk|Learn Apache Airf...|      Darshil Parmar|2023-10-07 13:00:04|Join My Data Engi...|[darshil parmar, ...|  2023-12-18|\n",
      "|7_HfVnjZ924|Installing Airflo...|       futureXskills|2023-01-14 19:47:29|Airflow Playlist ...|[airflow, apache ...|  2023-12-18|\n",
      "|DzxtCxi4YaA|Data Engineer Pro...|      Data with Marc|2023-08-08 14:11:37|Data Engineer Pro...|[data engineer, d...|  2023-12-18|\n",
      "|GqAcTrqKcrY|Realtime Data Str...|          CodeWithYu|2023-09-06 09:28:13|In this video, yo...|[Data Engineering...|  2023-12-18|\n",
      "|InDXPeg6r5U|How to Read and W...|Cloud & AI Analytics|2022-03-05 15:29:21|github url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|Jj6mp7Sam10|Dataproc in a minute|   Google Cloud Tech|2021-02-08 05:00:15|Learn more about ...|[GDS: Yes, what i...|  2023-12-18|\n",
      "|K9AnJ9_ZAXE|Airflow Tutorial ...|             coder2j|2022-06-05 22:00:07|Airflow Tutorial ...|[python, etl, dat...|  2023-12-18|\n",
      "|LkGFyi8S4Ys|Create a Dataproc...|Learn Cloud Compu...|2023-03-03 21:03:46|Please download t...|[Dataproc, Airflo...|  2023-12-18|\n",
      "|MhCuxTDlVkE|Airflow with DBT ...|      Data with Marc|2023-02-14 14:00:11|Airflow with DBT ...|[apache airflow, ...|  2023-12-18|\n",
      "|OBRv3t697sQ|GCP Composer | Ai...|Anjan GCP Data En...|2022-12-23 05:00:08|Code for educatio...|[gap, composer, a...|  2023-12-18|\n",
      "|PCg9AQNuK3E|Cloud Composer - ...|Cloud & AI Analytics|2023-03-05 19:54:42|GitHub url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|Q9eVezRq5pM|How to Read and W...|Cloud & AI Analytics|2022-12-23 10:08:32|GitHub url - http...|[Data Science, Go...|  2023-12-18|\n",
      "|TbdRQ-TmtgI|Apache Spark in 6...|      IBM Technology|2022-11-07 16:00:30|Learn more about ...|    [IBM, IBM Cloud]|  2023-12-18|\n",
      "|YgodScEIbOc|Create first Dag ...|        TechTrapture|2023-01-05 17:10:09|#gcp #googlecloud...|[composer, airflo...|  2023-12-18|\n",
      "|ZWgWiUH40qg|Apache Airflow Se...|              dataNX|2020-06-30 14:30:11|Hello,\\n\\nIn this...|[run airflow dag,...|  2023-12-18|\n",
      "|_lwrfxE2RtE|Google DataProc P...|           SkillCurb|2022-08-18 02:45:58|In this lab we wi...|[what is Dataproc...|  2023-12-18|\n",
      "|bHudgNDyltI|How to Read Diffe...|Cloud & AI Analytics|2022-05-16 23:33:35|github url: https...|[Data Science, Go...|  2023-12-18|\n",
      "|d4cu_rzv4A8|GCP Cloud Compose...|Anjan GCP Data En...|2022-11-08 16:21:05|GCP Cloud Compose...|[GCP, composer, a...|  2023-12-18|\n",
      "|dLcDBs9ilf0|How to Run Data S...|   Google Cloud Tech|2023-04-07 16:26:49|Are you trying to...|[dataproc, compos...|  2023-12-18|\n",
      "+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the base path of the partitioned table\n",
    "base_path = \"/work/rc/g.pillai/learnSpark/twitter-airflow/youtube_sample_data\"\n",
    "\n",
    "# Read the entire partitioned table\n",
    "df = spark.read.parquet(base_path)\n",
    "\n",
    "# Show the data in the DataFrame\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cfa2b47-c475-4bab-b14e-24fa66ddd7c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cafb3c-71f3-4605-944b-de8ac1cada7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
